## Package Choices

![](www/pillars-2.png)

::: {.notes}

- We just talked about package versions, but let's talk about choosing which package to use at all
- In the pillar of Package Choices, empathy means syntax similarity for easier code review and collaboration.

:::

## Package Choices

![](www/michael-speech-bubble.png)

::: {.notes}

- Here's the question I'm always asking myself: "How easy would it be for someone who knows R to review my Python code (and vice versa)?"
- The choice of packages you use can really impact how successful your multilingual data science team will be

:::

## Package Choices: dplyr & polars {transition="none-out"}

::: {.columns}
::: {.column}

```{.r}
library(dplyr)

df <-
    mtcars |>
        filter(
            mpg > 20
        ) |>
        select(wt) |>
        arrange(desc(wt)) |>
        head(5)
#
```

![](www/dplyr-logo.png)

:::
::: {.column}

```{.python}
import polars as pl

df = (
    mtcars
    .filter(
        (pl.col("mpg") > 20)
    )
    .select("wt")
    .sort("wt", descending=True)
    .head(5)
)
```

![](www/polars-logo.png)

:::
:::

::: {.notes}

- In these next few slides, you'll see R code examples on the left side, and Python code examples on the right side of the screen
- This goes somewhat back to my slide where I mentioned that everything great in each language had been ported to the other language, but it somewhat goes beyond that; I think the great *ideas* are being borrowed from each language
- The python package {polars} is a great example. The author of polars, Ritchie Vink, has mentioned in a bunch of interviews that {polars} takes a lot inspiration from the tidyverse in R
- We know that data prep is 80% of data science, so the methodology and opinions you choose for your data prep code is important
- At Ketchbrook, our choice is typically {polars} for Python data prep or {dplyr} for R data prep, because, not only are they two of the most popular data wrangling libraries in their respective languages, but they are also syntactically *very* similar

:::

## Package Choices: dplyr & polars {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="5-7"}
library(dplyr)

df <-
    mtcars |>
        filter(
            mpg > 20
        ) |>
        select(wt) |>
        arrange(desc(wt)) |>
        head(5)
#
```

:::
::: {.column}

```{.python code-line-numbers="5-7"}
import polars as pl

df = (
    mtcars
    .filter(
        (pl.col("mpg") > 20)
    )
    .select("wt")
    .sort("wt", descending=True)
    .head(5)
)
```

:::
:::

::: {.notes}

- You want to filter data in your data frame? Use the "filter" function in either package and specify the column and its constraint.

:::

## Package Choices: dplyr & polars {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="8"}
library(dplyr)

df <-
    mtcars |>
        filter(
            mpg > 20
        ) |>
        select(wt) |>
        arrange(desc(wt)) |>
        head(5)
#
```

:::
::: {.column}

```{.python code-line-numbers="8"}
import polars as pl

df = (
    mtcars
    .filter(
        (pl.col("mpg") > 20)
    )
    .select("wt")
    .sort("wt", descending=True)
    .head(5)
)
```

:::
:::

::: {.notes}

- Need to select a specific column or columns?  Use the "select" verb.

:::

## Package Choices: dplyr & polars {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="9"}
library(dplyr)

df <-
    mtcars |>
        filter(
            mpg > 20
        ) |>
        select(wt) |>
        arrange(desc(wt)) |>
        head(5)
#
```

:::
::: {.column}

```{.python code-line-numbers="9"}
import polars as pl

df = (
    mtcars
    .filter(
        (pl.col("mpg") > 20)
    )
    .select("wt")
    .sort("wt", descending=True)
    .head(5)
)
```

:::
:::

::: {.notes}

- And because nothing is *completely* perfect, we have the "arrange" verb in dplyr and "sort" in polars, with some slight differences in direction such that "descending" is a *function* in dplyr while it's an *argument* in polars.

:::

## Package Choices: dplyr & polars {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="10"}
library(dplyr)

df <-
    mtcars |>
        filter(
            mpg > 20
        ) |>
        select(wt) |>
        arrange(desc(wt)) |>
        head(5)
#
```

:::
::: {.column}

```{.python code-line-numbers="10"}
import polars as pl

df = (
    mtcars
    .filter(
        (pl.col("mpg") > 20)
    )
    .select("wt")
    .sort("wt", descending=True)
    .head(5)
)
```

:::
:::

::: {.notes}

- Lastly, we have the trusty "head" function from base R & polars for grabbing the first 5 rows of data after the other upstream ETL takes place
- Now {polars} is already pretty good at this, but in situations where we need to manipulate "big data" and {dplyr} isn't providing enough horsepower, you could opt for the {arrow} R package to continue to leverage a lot of {dplyr} syntax, but there's another interesting option that's perhaps even *more* language agnostic, that's actually courtesy of a Posit Conf keynote speaker last year...

:::

## Package Choices: DuckDB

![](www/duckdb-logo.png)

::: {.notes}

- If you haven't heard of it, DuckDB is a zero-dependency analytical database engine that essentially lets you do data wrangling in SQL at lightning-fast speed
- I'm underselling it -- if you haven't tried it, I highly encourage you to check it out.

:::

## Package Choices: DuckDB {transition="none-out"}

::: {.columns}
::: {.column}

```{.r}
library(duckdb)

# Connect to DuckDB (in-memory db)
conn <- dbConnect(duckdb::duckdb())

# Define SQL query
query <- "
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"

# Execute query and get results
result <- dbGetQuery(conn, query)
```

:::
::: {.column}

```{.python}
import duckdb

# Connect to DuckDB (in-memory db)
conn = duckdb.connect()

# Define SQL query
query = """
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"""

# Execute query and get results
result = conn.execute(query).df()
```

:::
:::

::: {.notes}

- But beyond that, we have great libraries in both R and Python that allow us to call DuckDB

:::

## Package Choices: DuckDB {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="4,16"}
library(duckdb)

# Connect to DuckDB (in-memory db)
conn <- dbConnect(duckdb::duckdb())

# Define SQL query
query <- "
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"

# Execute query and get results
result <- dbGetQuery(conn, query)
```

:::
::: {.column}

```{.python code-line-numbers="4,16"}
import duckdb

# Connect to DuckDB (in-memory db)
conn = duckdb.connect()

# Define SQL query
query = """
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"""

# Execute query and get results
result = conn.execute(query).df()
```

:::
:::

::: {.notes}

- In each language, there's a bit of setup in the front (to connect to the database) and some small syntactic differences at the end of the script...

:::

## Package Choices: DuckDB {transition="none"}

::: {.columns}
::: {.column}

```{.r code-line-numbers="7-13"}
library(duckdb)

# Connect to DuckDB (in-memory db)
conn <- dbConnect(duckdb::duckdb())

# Define SQL query
query <- "
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"

# Execute query and get results
result <- dbGetQuery(conn, query)
```

:::
::: {.column}

```{.python code-line-numbers="7-13"}
import duckdb

# Connect to DuckDB (in-memory db)
conn = duckdb.connect()

# Define SQL query
query = """
    SELECT wt
    FROM mtcars
    WHERE mpg > 20
    ORDER BY wt DESC
    LIMIT 5
"""

# Execute query and get results
result = conn.execute(query).df()
```

:::
:::

::: {.notes}

- But everything in the middle is the same!  It's 2025 and SQL will not die -- in fact it's as hot as ever!
- Using DuckDB is a great way to still have a *heavyweight* analytics engine that's easily portable across languages. If I had longer version of this data wrangling script in R that I needed to switch to Python for some reason, it would be extremely easy to do so -- I would only have to slightly alter the code at the very beginning and the very end.

:::

## Package Choices: ggplot2 & plotnine

::: {.columns}
::: {.column}

```{.r}
library(ggplot2)


ggplot(
  mtcars,
  aes(x = wt, y = mpg)
) +
  geom_point(color = "red") +
  geom_smooth(method = "lm")
#
```

:::
::: {.column}

```{.python}
from plotnine import

(
  ggplot(
    mtcars,
    aes(x = "wt", y = "mpg")
  ) +
    geom_point(color = "red") +
    geom_smooth(method = "lm")
)
```

:::
:::

::: {.notes}

- Let's take a look at another example: {ggplot2} in R and {plotnine} in Python
- With the exception of quoting column names and wrapping the entire expression in parentheses in Python, the code is *literally* identical

:::

## Package Choices: gt & great-tables

::: {.columns}
::: {.column}

```{.r}
library(gt)

int_cols <- c(
  "cyl", "vs", "am",
  "gear", "carb"
)


gt(mtcars) |>
  tab_header(
    title = "Awesome mtcars",
    subtitle = "With ðŸ’™ + GT"
  ) |>
  fmt_number() |>
  fmt_integer(columns = int_cols)
#
```

:::
::: {.column}

```{.python}
from great_tables import GT

int_cols = [
  "cyl", "vs", "am",
  "gear", "carb"
]

(
  GT(mtcars)
  .tab_header(
      title = "Awesome mtcars",
      subtitle = "With ðŸ’™ + GT"
  .) |>
  .fmt_number() |>
  .fmt_integer(columns=int_cols)
)
```

:::
:::

::: {.notes}

- One last example, {gt} in R versus {great-tables} in Python
- These last two examples are no accident -- Posit is behind all of these four packages: ggplot2, plotnine, gt, and great-tables; and they have put a lot of work into structuring these packages in a way that preserves a ton of syntax similarity

:::